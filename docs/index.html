<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DisMo learns abstract motion representations that enable open-world motion transer">
  <meta property="og:title" content="DisMo: Disentangled Motion Representations for Open-World Motion Transfer" />
  <meta property="og:description"
    content="DisMo learns abstract motion representations that enable open-world motion transer" />
  <meta property="og:url" content="" />

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="https://github.com/tomresan/dismo-release/docs/static/images/teaser.png" />
  <meta property="og:image:width" content="939" />
  <meta property="og:image:height" content="494" />


  <meta name="twitter:title" content="DisMo: Disentangled Motion Representations for Open-World Motion Transfer">
  <meta name="twitter:description"
    content="DisMo learns abstract motion representations that enable open-world motion transer">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="https://github.com/tomresan/dismo-release/docs/static/images/teaser.png">
  <meta name="twitter:card" content="DisMo learns abstract motion representations that enable open-world motion transer">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Motion Understanding, Motion Transfer, Video Representations, Generative Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 32 32%22><text y=%221em%22 font-size=%2232%22>ðŸ“Œ</text></svg>">
  <title>DisMo: Disentangled Motion Representations for Open-World Motion Transfer</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">


  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/twentytwenty.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="static/js/jquery-3.2.1.min.js"></script>
  <script src="static/js/jquery.event.move.js"></script>
  <script src="static/js/jquery.twentytwenty.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"> <i>DisMo:</i> Disentangled Motion Representations for Open-World Motion Transfer</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/thomas-ressler-494758133/" target="_blank">Thomas Ressler-Antal</a> Â· </span>
              <span class="author-block">
                <a href="https://ffundel.de/" target="_blank">Frank Fundel</a><sup>*</sup> Â· </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/malek-ben-alaya/" target="_blank">Malek Ben Alaya</a><sup>*</sup></span>
              <br>
              <span class="author-block">
                <a href="https://stefan-baumann.eu/" target="_blank">Stefan A. Baumann</a> Â· </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/felixmkrause/" target="_blank">Felix Krause</a> Â· </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/ming-gui-87b76a16b/" target="_blank">Ming Gui</a> Â· </span>
              <span class="author-block">
                <a href="https://ommer-lab.com/people/ommer/" target="_blank">BjÃ¶rn Ommer</a></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"> <b>CompVis @ LMU Munich, MCML</b>
                <br>
                <i>* equal contribution</i>
              </span>
              <!-- <span class="eql-cntrb"><sup>*</sup>Indicates Equal Contribution</span> -->
            </div>
            <div class="is-size-5 publication-venue">
              <span>NeurIPS 2025 Spotlight</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!--  PDF link -->
                <span class="link-block">
                  <a href="static/dismo.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>


                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/CompVis/dismo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->

                <span class="link-block">
                  <a href="https://openreview.net/forum?id=jneVld5iZw" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>


                <!-- HF weights Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/CompVis/dismo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face"
                        style="height: 20px;">
                    </span>
                    <span>Weights</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">

            <img src="static/images/teaser.png" alt="" style="width: 100%; height: auto;">
          </div>

        </div>

        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="content has-text-justified">
                <p style="margin-bottom: 20px; margin-top: 10px;">
                  <span style="font-weight: bold">TL;DR:</span> Our Flow Poke Transformer (FPT) directly models the uncertainty of the world by predicting distributions of how objects (<span style="color:#ff7f0e">Ã—</span>) may move conditioned on some  input movements (pokes, â†’). We see that whether the hand (below paw) or the paw (above hand) moves downwards directly influences the other's movement. Left: the paw pushing the hand down, will force the hand downwards, resulting in a unimodal distribution. Right: the hand moving down results in two modes, the paw following along or staying put.
                </p>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p class="has-text-justified" style="font-size: 16px;">
              Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose <i><b>DisMo</b></i>, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->



  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
            <img src="static/images/architecture.svg" alt="" style="width: 100%; height: auto; margin-bottom: 20px;">
            <p class="has-text-justified" style="font-size: 16px;">
              \textbf{(a)} During training, our motion extractor $\motionextractor$ receives augmented frames from a video $\video$, along with additional motion query tokens $\mathbf{Q}$. These are then individually passed to the frame generator $\framegenerator$, alongside the corresponding source frame \(\videoframe_t\), from which it learns to reconstruct a frame at a future timestep $\timedestination$. 
    \textbf{(b)} To transfer a motion sequence $\motionsequence$ onto another target image, we can directly utilize the trained Frame Generator $\mathcal{F}_{\psi}$ autoregressively as a low-cost option.
    \textbf{(c)} For high-quality motion transfer, we adapt pre-trained off-the-shelf video generation models. A motion sequence is first embedded using a mapping network, before being introduced to the frozen video model. The processed motion sequence is arranged such that each token at timestep \(t\) in the pre-trained backbone receives conditioning only from the temporally corresponding motion embedding $\motion_t$.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Comparison and Capabilities</h2>
          <div class="content">
            <h3 class="title">Face Motion Generation</h3>
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <img src="static/images/face_motion.png" alt="" style="width: 100%; height: auto;">
              </div>
            </div>
            <p class="has-text-justified" style="font-size: 16px;">
              We show fine-grained zero-shot poking results on faces and compare against InstantDrag, which was trained specifically for this task. We further visualize the predicted motion as warps using InstantDrag's face warping model. The qualitative results show that our model tends to predict more accurate and localized motion.
            </p>
            <h3 class="title">Moving Part Segmentation</h3>
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <img src="static/images/openset_mov_segment.png" alt="" style="width: 100%; height: auto;">
              </div>
            </div>
            <p class="has-text-justified" style="font-size: 16px;">
              We perform moving part segmentation with our method by thresholding the KL divergence between the pointwise unconditional motion distribution and the pointwise motion distribution conditioned on a specific poke. Our method shows strong moving part segmentation performance in generic open-set cases.
            </p>
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <img src="static/images/dragapart.png" alt="" style="width: 100%; height: auto;">
              </div>
            </div>
            <p class="has-text-justified" style="font-size: 16px;">
              We directly replicate Fig. 7 from DragAPart [21] with our method. Our method provides spatially continuous predictions and makes fewer critical mistakes like segmenting the furniture body with the drawer (top right). Quantitatively, we find that our method, especially when finetuned in-domain, outperforms DragAPart, which introduced this benchmark.
            </p>
            <h3 class="title">Articulated Motion Estimation</h3>
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <img src="static/images/dragamove.png" alt="" style="width: 100%; height: auto;">
              </div>
            </div>
            <p class="has-text-justified" style="font-size: 16px;">
              We compare on the Drag-A-Move dataset with Motion-I2V, DragAPart, and PuppetMaster. Our (finetuned?) model is qualitatively more capable of capturing complex conditioning with multiple different pokes than DAP and PM in this setup. Motion-I2V often fails to accurately follow the conditioning locally.
            </p>
            <h3 class="title">Unconditional Sampling</h3>
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <img src="static/images/uncond.png" alt="" style="width: 100%; height: auto;">
              </div>
            </div>
            <p class="has-text-justified" style="font-size: 16px;">
              By sampling autoregressively without an initial poke, we can sample from the joint distribution of movement of the whole scene. We show such samples of generated flow without prior motion conditioning on pokes. Our model can generate a wide variety of realistic motions.

            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{resslerdismo,
  title={DisMo: Disentangled Motion Representations for Open-World Motion Transfer},
  author={Ressler-Antal, Thomas and Fundel, Frank and Alaya, Malek Ben and Baumann, Stefan Andreas and Krause, Felix and Gui, Ming and Ommer, Bj{\"o}rn},
  booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
  year={2025}
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

  <script>
    document.addEventListener('DOMContentLoaded', () => {

      // Get all "navbar-burger" elements
      const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

      // Add a click event on each of them
      $navbarBurgers.forEach(el => {
        el.addEventListener('click', () => {

          // Get the target from the "data-target" attribute
          // const target = el.dataset.target;
          // const $target = document.getElementById(target);
          menu = el.parentElement.parentElement.children[1]

          // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
          el.classList.toggle('is-active');
          menu.classList.toggle('is-active');

        });
      });

    });
  </script>

</body>

</html>