<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DisMo learns abstract motion representations that enable open-world motion transer">
  <meta property="og:title" content="DisMo: Disentangled Motion Representations for Open-World Motion Transfer" />
  <meta property="og:description"
    content="DisMo learns abstract motion representations that enable open-world motion transer" />
  <meta property="og:url" content="" />

  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
  <meta property="og:image" content="https://github.com/tomresan/dismo-release/docs/static/images/teaser.png" />
  <meta property="og:image:width" content="939" />
  <meta property="og:image:height" content="494" />


  <meta name="twitter:title" content="DisMo: Disentangled Motion Representations for Open-World Motion Transfer">
  <meta name="twitter:description"
    content="DisMo learns abstract motion representations that enable open-world motion transer">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="https://github.com/tomresan/dismo-release/docs/static/images/teaser.png">
  <meta name="twitter:card" content="DisMo learns abstract motion representations that enable open-world motion transer">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Motion Understanding, Motion Transfer, Video Representations, Generative Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 32 32%22><text y=%221em%22 font-size=%2232%22></text></svg>">
  <title>DisMo: Disentangled Motion Representations for Open-World Motion Transfer</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">


  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/twentytwenty.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="static/js/jquery-3.2.1.min.js"></script>
  <script src="static/js/jquery.event.move.js"></script>
  <script src="static/js/jquery.twentytwenty.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"> <i>DisMo:</i> Disentangled Motion Representations for Open-World Motion Transfer</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/thomas-ressler-494758133/" target="_blank">Thomas Ressler-Antal</a> 路 </span>
              <span class="author-block">
                <a href="https://ffundel.de/" target="_blank">Frank Fundel</a><sup>*</sup> 路 </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/malek-ben-alaya/" target="_blank">Malek Ben Alaya</a><sup>*</sup></span>
              <br>
              <span class="author-block">
                <a href="https://stefan-baumann.eu/" target="_blank">Stefan A. Baumann</a> 路 </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/felixmkrause/" target="_blank">Felix Krause</a> 路 </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/ming-gui-87b76a16b/" target="_blank">Ming Gui</a> 路 </span>
              <span class="author-block">
                <a href="https://ommer-lab.com/people/ommer/" target="_blank">Bj枚rn Ommer</a></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"> <b>CompVis @ LMU Munich, MCML</b>
                <br>
                <i>* equal contribution</i>
              </span>
              <!-- <span class="eql-cntrb"><sup>*</sup>Indicates Equal Contribution</span> -->
            </div>
            <div class="is-size-5 publication-venue">
              <span>NeurIPS 2025 Spotlight</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!--  PDF link -->
                <span class="link-block">
                  <a href="static/dismo.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>


                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/CompVis/dismo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->

                <span class="link-block">
                  <a href="https://openreview.net/forum?id=jneVld5iZw" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>


                <!-- HF weights Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/CompVis/dismo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face"
                        style="height: 20px;">
                    </span>
                    <span>Weights</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <img src="static/images/teaser.png" alt="" style="width: 100%; height: auto;">
            </div>
          </div>
        </div>

        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="content has-text-justified">
                <p style="margin-bottom: 20px; margin-top: 10px;">
                  <span style="font-weight: bold">TL;DR:</span> We present <b>DisMo</b>, a paradigm that learns a semantic motion representation space from videos that is disentangled from static content information such as appearance, structure, viewing angle and even object category. We leverage this invariance and condition off-the-shelf video models on extracted motion embeddings. This setup achieves state-of-the-art performance on open-world motion transfer with a high degree of transferability in cross-category and -viewpoint settings. Beyond that, DisMo's learned representations are suitable for downstream tasks such as zero-shot action classification.
                </p>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p class="has-text-justified" style="font-size: 16px;">
              Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose <i><b>DisMo</b></i>, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. <br><br>Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->




  <!-- Video Results -->
  <section class="section hero" id="results">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Motion Transfer</h2>
          <p class="has-text-justified" style="font-size: 16px; margin-bottom: 2rem;">
            We present qualitative results of <b>open-world motion transfer</b>. Each video shows the
            <b>source motion</b> on the left and <b>generated targets</b> on the right.
          </p>
        </div>
      </div>

      <!-- Result 1 -->
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <h3 class="title is-5">Appearance-invariant transfer</h3>
          <video class="video-result" autoplay muted loop playsinline controls>
            <source src="static/videos/transfer_03.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p class="has-text-grey" style="margin-top: 0.75rem;">
            Different targets share the same motion while retaining their individual appearance and structure.
          </p>
        </div>
      </div>


      <!-- Result 2 -->
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <h3 class="title is-5">Cross-viewpoint motion transfer</h3>
          <video class="video-result" autoplay muted loop playsinline controls>
            <source src="static/videos/cross_viewpoint_transfers.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p class="has-text-grey" style="margin-top: 0.75rem;">
            Source motion captured from a front-facing viewpoint, transferred to oblique targets.
          </p>
        </div>
      </div>


      <!-- Result 3 -->
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <h3 class="title is-5">Cross-category motion transfer</h3>
          <video class="video-result" autoplay muted loop playsinline controls>
            <source src="static/videos/cross_category_transfers.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p class="has-text-grey" style="margin-top: 0.75rem;">
            Motion from a human subject transferred to a non-human target while preserving motion semantics.
          </p>
        </div>
      </div>


      <!-- Result 4 -->
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <h3 class="title is-5">Camera transfer</h3>
          <video class="video-result" autoplay muted loop playsinline controls>
            <source src="static/videos/camera_transfers.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <p class="has-text-grey" style="margin-top: 0.75rem;">
            Camera trajectories are tranferred to vastly different target scenes.
          </p>
        </div>
      </div>

    </div>
  </section>
  <!-- End Video Results -->




  <!-- Method -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
            <img src="static/images/architecture.svg" alt="" style="width: 100%; height: auto; margin-bottom: 20px;">
            <p class="has-text-justified" style="font-size: 16px;">
              <b>(a)</b> Our motion extractor receives augmented video frames and outputs a motion embedding for each, which represents the higher-level temporal dynamics at that time. These are then individually passed to a frame generator, alongside the corresponding source frame, from which it learns to reconstruct a frame at a future timestep. 
              <b>(b)</b> To transfer a sequence of motion embeddings onto another target image, we can directly utilize the trained frame generator autoregressively as a low-cost option.
              <b>(c)</b> For high-quality motion transfer, we adapt pre-trained off-the-shelf video generation models. A motion sequence is first embedded using a mapping network, before being introduced to the frozen video model. The processed motion sequence is arranged such that each token at a specific timestep in the pre-trained backbone receives conditioning only from the temporally corresponding motion embedding.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Method -->




    <!-- Additional Video Results -->
  <section class="section hero" id="results">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">More Examples</h2>
        </div>
      </div>

      <!-- Additional Result 1 -->
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <h3 class="title is-5">Appearance-invariant transfer</h3>
          <video class="video-result" autoplay muted loop playsinline controls>
            <source src="static/videos/additional_transfer_01.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>

      <!-- Additional Result 2 -->
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <h3 class="title is-5">Appearance-invariant transfer</h3>
          <video class="video-result" autoplay muted loop playsinline controls>
            <source src="static/videos/additional_transfer_02.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>

      <!-- Additional Result 3 -->
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <h3 class="title is-5">Appearance-invariant transfer</h3>
          <video class="video-result" autoplay muted loop playsinline controls>
            <source src="static/videos/additional_transfer_03.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>

      <!-- Additional Result 4 -->
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <h3 class="title is-5">Appearance-invariant transfer</h3>
          <video class="video-result" autoplay muted loop playsinline controls>
            <source src="static/videos/additional_transfer_04.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>

      <!-- Additional Result 5 -->
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <h3 class="title is-5">Appearance-invariant transfer</h3>
          <video class="video-result" autoplay muted loop playsinline controls>
            <source src="static/videos/additional_transfer_05.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>

    </div>
  </section>
  <!-- End Video Results -->




  <!-- Quantitative Motion Transfer Evaluation -->
  <section class="section hero" id="quantitative">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Quantitative Motion Transfer Evaluation</h2>
          <p class="has-text-justified" style="font-size: 16px; margin-bottom: 1.5rem;">
            We quantitatively evaluate <b>open-world motion transfer</b> using both automatic metrics and human studies.
            While prior methods exhibit a clear trade-off between <b>motion fidelity</b> and <b>prompt adherence</b>,
            <b>DisMo</b> achieves <i>state-of-the-art performance</i> on both simultaneously, breaking the usual
            compromise.
          </p>

          <figure class="metric-figure">
            <img src="static/images/motion_transfer_eval_automated.svg"
                 alt="Quantitative motion transfer evaluation with automatic and human metrics where DisMo outperforms all baselines without a trade-off between motion fidelity and prompt adherence.">
            <figcaption>
              <b>Figure:</b> Automatic and human evaluation of motion transfer. Unlike existing methods, which trade
              off motion fidelity and prompt adherence, <b>DisMo</b> is best in both metrics without sacrificing either.
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </section>
  <!-- End Quantitative Motion Transfer Evaluation -->





  <!-- Disentangled Motion Embeddings -->
  <section class="section hero is-light" id="disentanglement">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Disentangled Motion Embeddings</h2>
          <p class="has-text-justified" style="font-size: 16px; margin-bottom: 1.5rem;">
            We analyze disentanglement using a <b>k-NN retrieval</b> setup where queries target either
            <b>action</b> or <b>identity</b>. For motion transfer, desirable neighbors share the same
            <i>action</i> but differ in identity. <b>DisMo</b> exhibits exactly this behavior: it retrieves videos
            with the same motion performed by different actors, while standard video representation learning baselines
            mostly retrieve the same identity performing different actions.
          </p>

          <figure class="metric-figure">
            <img src="static/images/disentanglement_eval.svg"
                 alt="k-NN retrieval evaluation showing that DisMo retrieves clips with the same action and different identities, while baselines focus on identity.">
            <figcaption>
              <b>Figure:</b> k-NN retrieval analysis. <b>DisMo</b> focuses on motion semantics (same action, different
              identity), whereas generic video representation baselines primarily capture identity and appearance.
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </section>
  <!-- End Disentangled Motion Embeddings -->





  <!-- Zero-Shot Action Classification -->
  <section class="section hero" id="zeroshot">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Zero-Shot Action Classification</h2>
          <p class="has-text-justified" style="font-size: 16px; margin-bottom: 1.5rem;">
            Beyond motion transfer, we demonstrate that <b>DisMo</b>'s motion embeddings are genuinely <b>semantic</b>.
            In a zero-shot action classification setup, where no task-specific fine-tuning is performed, DisMo
            outperforms video representation baselines on multiple datasets.
            This shows that DisMo captures <b>high-level temporal dynamics</b> that are directly useful for downstream
            motion understanding tasks.
          </p>

          <figure class="metric-figure">
            <img src="static/images/knn_action_classification_eval.svg"
                 alt="Zero-shot action classification results showing DisMo outperforming other video representation baselines.">
            <figcaption>
              <b>Figure:</b> Zero-shot action classification on benchmarks such as Something-Something v2 and Jester.
              <b>DisMo</b> achieves superior performance, indicating that its motion representations encode
              meaningful action semantics that can be leveraged beyond motion transfer.
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </section>
  <!-- End Zero-Shot Action Classification -->





  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{resslerdismo,
  title={DisMo: Disentangled Motion Representations for Open-World Motion Transfer},
  author={Ressler-Antal, Thomas and Fundel, Frank and Alaya, Malek Ben and Baumann, Stefan Andreas and Krause, Felix and Gui, Ming and Ommer, Bj{\"o}rn},
  booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
  year={2025}
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the<a
                href="https://nerfies.github.io" target="_blank">Nerfies</a>project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

  <script>
    document.addEventListener('DOMContentLoaded', () => {

      // Get all "navbar-burger" elements
      const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

      // Add a click event on each of them
      $navbarBurgers.forEach(el => {
        el.addEventListener('click', () => {

          // Get the target from the "data-target" attribute
          // const target = el.dataset.target;
          // const $target = document.getElementById(target);
          menu = el.parentElement.parentElement.children[1]

          // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
          el.classList.toggle('is-active');
          menu.classList.toggle('is-active');

        });
      });

    });
  </script>

</body>

</html>